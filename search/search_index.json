{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Di Halaman Tugas Data Mining \u00b6 Biodata \u00b6 `Nama : Risky Sulistiyani. `NIM : 180411100036. `Kelas : Penambangan Data 5D. `Angkatan : 2018. Dosen Pengampu : Mula'ab,S.si.,M.Kom Alamat : Jakarta","title":"index"},{"location":"#welcome-di-halaman-tugas-data-mining","text":"","title":"Welcome Di Halaman Tugas Data Mining"},{"location":"#biodata","text":"`Nama : Risky Sulistiyani. `NIM : 180411100036. `Kelas : Penambangan Data 5D. `Angkatan : 2018. Dosen Pengampu : Mula'ab,S.si.,M.Kom Alamat : Jakarta","title":"Biodata"},{"location":"DECISION%20TREE/","text":"DECISION TREE \u00b6 decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"DECISION TREE"},{"location":"DECISION%20TREE/#decision-tree","text":"decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE"},{"location":"DECISION%20TREE/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"CARA MEMBUAT DECISION TREE"},{"location":"Eliminasi%20Gauss/","text":"Eliminasi Gauss Jordan \u00b6 \u200b Eliminasi Gauss adalah suatu metode untuk mengoperasikan nilai-nilai di dalam matriks sehingga menjadi matriks yang lebih sederhana lagi. Dengan melakukan operasi baris sehingga matriks tersebut menjadi matriks yang baris. Ini dapat digunakan sebagai salah satu metode penyelesaian persamaan linear dengan menggunakan matriks. Caranya dengan mengubah persamaan linear tersebut ke dalam matriks teraugmentasi dan mengoperasikannya. Setelah menjadi matriks baris, lakukan substitusi balik untuk mendapatkan nilai dari variabel-variabel tersebut. \u200b Metode Eliminasi Gauss Jordan merupakan pengembangan metode eliminasi gauss, hanya saja augmented matrik , pada sebelah kiri dirubah menjadi matrik diagonal. Algoritma Gauss Jordan \u00b6 \u00b6 Listing Program \u00b6 import numpy as np #Definisi Matrix A = [] B = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) A . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) B . append ( h ) Matrix = np . array ( A , float ) Hasil = np . array ( B , float ) n = len ( Matrix ) #Eliminasi Gauss for k in range ( 0 , n - 1 ): for i in range ( k + 1 , n ): if Matrix [ i , k ] != 0 : lam = Matrix [ i , k ] / Matrix [ k , k ] Matrix [ i , k : n ] = Matrix [ i , k : n ] - ( Matrix [ k , k : n ] * lam ) Hasil [ i ] = Hasil [ i ] - ( Hasil [ k ] * lam ) print ( \"Matrix A : \" , ' \\n ' , Matrix ) #Subtitution x = np . zeros ( n , float ) for m in range ( n - 1 , - 1 , - 1 ): x [ m ] = ( Hasil [ m ] - np . dot ( Matrix [ m , m + 1 : n ], x [ m + 1 : n ])) / Matrix [ m , m ] print ( 'Nilai X ' , m + 1 , '=' , x [ m ]) Output: Masukkan ukuran Matrix: 3 Masukkan Nilai: 2 Masukkan Nilai: -2 Masukkan Nilai: 5 Masukkan Nilai: 1 Masukkan Nilai: 5 Masukkan Nilai: 2 Masukkan Nilai: 4 Masukkan Nilai: 5 Masukkan Nilai: 2 Masukkan Hasil: 12 Masukkan Hasil: 3 Masukkan Hasil: -4 Matrix A : [[ 2. -2. 5. ] [ 0. 6. -0.5 ] [ 0. 0. -7.25]] Nilai X 3 = 3.2413793103448274 Nilai X 2 = -0.2298850574712644 Nilai X 1 = -2.333333333333332 jadi panjang Matrix yang dibuat dalam Program Diatas adalah 3 variabel. |2 -2 5| |12| |1 5 2|=| 3 | |4 5 2| |-4| pivot yang dibentuk adalah a1.1,a2.2,dan a3.3 sehingga semua angka yang ada dibawah pivot akan dikonversikan menjadi nol sesuai hasil program dan hasil dari persamaan diatas menghasilkan x1=-2.333333333, x2=-0.22988505 dan x3=3.2413793 Eliminasi Gauss Jacobi \u00b6 Metode IterasiJacobi merupakan salah satu bidang analisis numerik yang digunakan untuk menyelesaikan permasalahan Persamaan Linier dan sering dijumpai dalam berbagai disiplin ilmu. Metode Iterasi Jacobi merupakan salah satu metode tak langsung, yaitu bermula dari suatu hampiran penyelesaian awal dan kemudian berusaha memperbaiki hampiran dalam tak berhingga namun langkah konvergen. Metode Iterasi Jacobi ini digunakan untuk menyelesaikan persamaan Linier berukuran besar dan proporsi koefisien nolnya besar. Metode ini ditemukan oleh Matematikawan yang berasal dari Jerman,Carl,Gustav,Jacobi. Penemuan ini diperkirakan pada tahun 1800-an. Listing Program \u00b6 from pprint import pprint from numpy import array , zeros , diag , diagflat , dot import numpy as np def jacobi ( A , b , N = 25 , x = None ): #Membuat iniial guess if x is None : x = zeros ( len ( A [ 0 ])) #Membuat vektor dari elemen matrix A D = diag ( A ) R = A - diagflat ( D ) #Iterasi for i in range ( N ): x = ( b - dot ( R , x )) / D return x Mat1 = [] Mat2 = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) Mat1 . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) Mat2 . append ( h ) A = array ( Mat1 , float ) b = array ( Mat2 , float ) x = len ( Mat1 ) guess = np . zeros ( x , float ) sol = jacobi ( A , b , N = 25 , x = guess ) print ( \"A:\" ) pprint ( A ) print ( \"b:\" ) pprint ( b ) print ( \"x:\" ) pprint ( sol ) Output: Masukkan ukuran Matrix: 3 Masukkan Nilai: 3 Masukkan Nilai: 1 Masukkan Nilai: -1 Masukkan Nilai: 4 Masukkan Nilai: 7 Masukkan Nilai: -3 Masukkan Nilai: 2 Masukkan Nilai: -2 Masukkan Nilai: 5 Masukkan Hasil: 5 Masukkan Hasil: 20 Masukkan Hasil: 10 A: array([[ 3., 1., -1.], [ 4., 7., -3.], [ 2., -2., 5.]]) b: array([ 5., 20., 10.]) x: array([1.50602413, 3.13253016, 2.6506024 ]) Program Gauss Seidel \u00b6 Listing Program \u00b6 def seidel ( a , x , b ): #Mencari Panjang Matrix n = len ( a ) for j in range ( 0 , n ): d = b [ j ] #Menghitung xi, yi, zi for i in range ( 0 , n ): if ( j != i ): d -= a [ j ][ i ] * x [ i ] x [ j ] = d / a [ j ][ j ] #Solusi return x m = int ( input ( \"Masukkan Panjang Matrix: \" )) a = [] b = [] for k in range ( m ): mat1 = [] for i in range ( m ): l = float ( input ( \"Masukkan a\" + str ( k + 1 ) + \",\" + str ( i + 1 ) + \": \" )) mat1 . append ( l ) h = float ( input ( \"Masukkan Hasil: \" )) b . append ( h ) a . append ( mat1 ) n = 3 x = [ 0 , 0 , 0 ] print ( x ) for i in range ( 0 , 100 ): x = seidel ( a , x , b ) print ( x ) Output: Masukkan Panjang Matrix: 3 Masukkan a1,1: 4 Masukkan a1,2: -1 Masukkan a1,3: 1 Masukkan Hasil: 7 Masukkan a2,1: 4 Masukkan a2,2: -8 Masukkan a2,3: 1 Masukkan Hasil: -21 Masukkan a3,1: -2 Masukkan a3,2: 1 Masukkan a3,3: 5 Masukkan Hasil: 15 [0, 0, 0] [1.75, 3.5, 3.0] [1.875, 3.9375, 2.9625] [1.99375, 3.9921875, 2.9990625] [1.99828125, 3.9990234375, 2.9995078125] [1.99987890625, 3.9998779296875, 2.9999759765625003] [1.99997548828125, 3.9999847412109375, 2.999993247070312] [1.9999978735351562, 3.9999980926513667, 2.999999530883789] [1.9999996404418945, 3.9999997615814205, 2.9999999038604734] [1.9999999644302369, 3.9999999701976776, 2.9999999917325595] [1.9999999946162794, 3.9999999962747097, 2.99999999859157] [1.9999999994207849, 3.9999999995343387, 2.9999999998614464] [1.9999999999182232, 3.999999999941793, 2.999999999978931] [1.9999999999907154, 3.999999999992724, 2.9999999999977414] [1.9999999999987457, 3.9999999999990905, 2.9999999999996803] [1.9999999999998526, 3.9999999999998863, 2.9999999999999636] [1.9999999999999807, 3.999999999999986, 2.999999999999995] [1.9999999999999978, 3.9999999999999987, 2.9999999999999996] [1.9999999999999996, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] Dari soal diatas persamaan yang dipilih adalah 4x-y+z=7, 4x-8y+z=-21 dan -2x+y+5z=15. Iterasi yang digunakan sebanyak 100 iterasi sehingga dapat menghasilkan x=2,y=4 dan z=3. Sekian dan Terima Kasih","title":"Eliminasi Gauss"},{"location":"Eliminasi%20Gauss/#eliminasi-gauss-jordan","text":"\u200b Eliminasi Gauss adalah suatu metode untuk mengoperasikan nilai-nilai di dalam matriks sehingga menjadi matriks yang lebih sederhana lagi. Dengan melakukan operasi baris sehingga matriks tersebut menjadi matriks yang baris. Ini dapat digunakan sebagai salah satu metode penyelesaian persamaan linear dengan menggunakan matriks. Caranya dengan mengubah persamaan linear tersebut ke dalam matriks teraugmentasi dan mengoperasikannya. Setelah menjadi matriks baris, lakukan substitusi balik untuk mendapatkan nilai dari variabel-variabel tersebut. \u200b Metode Eliminasi Gauss Jordan merupakan pengembangan metode eliminasi gauss, hanya saja augmented matrik , pada sebelah kiri dirubah menjadi matrik diagonal.","title":"Eliminasi Gauss Jordan"},{"location":"Eliminasi%20Gauss/#algoritma-gauss-jordan","text":"","title":"Algoritma Gauss Jordan"},{"location":"Eliminasi%20Gauss/#_1","text":"","title":""},{"location":"Eliminasi%20Gauss/#listing-program","text":"import numpy as np #Definisi Matrix A = [] B = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) A . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) B . append ( h ) Matrix = np . array ( A , float ) Hasil = np . array ( B , float ) n = len ( Matrix ) #Eliminasi Gauss for k in range ( 0 , n - 1 ): for i in range ( k + 1 , n ): if Matrix [ i , k ] != 0 : lam = Matrix [ i , k ] / Matrix [ k , k ] Matrix [ i , k : n ] = Matrix [ i , k : n ] - ( Matrix [ k , k : n ] * lam ) Hasil [ i ] = Hasil [ i ] - ( Hasil [ k ] * lam ) print ( \"Matrix A : \" , ' \\n ' , Matrix ) #Subtitution x = np . zeros ( n , float ) for m in range ( n - 1 , - 1 , - 1 ): x [ m ] = ( Hasil [ m ] - np . dot ( Matrix [ m , m + 1 : n ], x [ m + 1 : n ])) / Matrix [ m , m ] print ( 'Nilai X ' , m + 1 , '=' , x [ m ]) Output: Masukkan ukuran Matrix: 3 Masukkan Nilai: 2 Masukkan Nilai: -2 Masukkan Nilai: 5 Masukkan Nilai: 1 Masukkan Nilai: 5 Masukkan Nilai: 2 Masukkan Nilai: 4 Masukkan Nilai: 5 Masukkan Nilai: 2 Masukkan Hasil: 12 Masukkan Hasil: 3 Masukkan Hasil: -4 Matrix A : [[ 2. -2. 5. ] [ 0. 6. -0.5 ] [ 0. 0. -7.25]] Nilai X 3 = 3.2413793103448274 Nilai X 2 = -0.2298850574712644 Nilai X 1 = -2.333333333333332 jadi panjang Matrix yang dibuat dalam Program Diatas adalah 3 variabel. |2 -2 5| |12| |1 5 2|=| 3 | |4 5 2| |-4| pivot yang dibentuk adalah a1.1,a2.2,dan a3.3 sehingga semua angka yang ada dibawah pivot akan dikonversikan menjadi nol sesuai hasil program dan hasil dari persamaan diatas menghasilkan x1=-2.333333333, x2=-0.22988505 dan x3=3.2413793","title":"Listing Program"},{"location":"Eliminasi%20Gauss/#eliminasi-gauss-jacobi","text":"Metode IterasiJacobi merupakan salah satu bidang analisis numerik yang digunakan untuk menyelesaikan permasalahan Persamaan Linier dan sering dijumpai dalam berbagai disiplin ilmu. Metode Iterasi Jacobi merupakan salah satu metode tak langsung, yaitu bermula dari suatu hampiran penyelesaian awal dan kemudian berusaha memperbaiki hampiran dalam tak berhingga namun langkah konvergen. Metode Iterasi Jacobi ini digunakan untuk menyelesaikan persamaan Linier berukuran besar dan proporsi koefisien nolnya besar. Metode ini ditemukan oleh Matematikawan yang berasal dari Jerman,Carl,Gustav,Jacobi. Penemuan ini diperkirakan pada tahun 1800-an.","title":"Eliminasi Gauss Jacobi"},{"location":"Eliminasi%20Gauss/#listing-program_1","text":"from pprint import pprint from numpy import array , zeros , diag , diagflat , dot import numpy as np def jacobi ( A , b , N = 25 , x = None ): #Membuat iniial guess if x is None : x = zeros ( len ( A [ 0 ])) #Membuat vektor dari elemen matrix A D = diag ( A ) R = A - diagflat ( D ) #Iterasi for i in range ( N ): x = ( b - dot ( R , x )) / D return x Mat1 = [] Mat2 = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) Mat1 . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) Mat2 . append ( h ) A = array ( Mat1 , float ) b = array ( Mat2 , float ) x = len ( Mat1 ) guess = np . zeros ( x , float ) sol = jacobi ( A , b , N = 25 , x = guess ) print ( \"A:\" ) pprint ( A ) print ( \"b:\" ) pprint ( b ) print ( \"x:\" ) pprint ( sol ) Output: Masukkan ukuran Matrix: 3 Masukkan Nilai: 3 Masukkan Nilai: 1 Masukkan Nilai: -1 Masukkan Nilai: 4 Masukkan Nilai: 7 Masukkan Nilai: -3 Masukkan Nilai: 2 Masukkan Nilai: -2 Masukkan Nilai: 5 Masukkan Hasil: 5 Masukkan Hasil: 20 Masukkan Hasil: 10 A: array([[ 3., 1., -1.], [ 4., 7., -3.], [ 2., -2., 5.]]) b: array([ 5., 20., 10.]) x: array([1.50602413, 3.13253016, 2.6506024 ])","title":"Listing Program"},{"location":"Eliminasi%20Gauss/#program-gauss-seidel","text":"","title":"Program Gauss Seidel"},{"location":"Eliminasi%20Gauss/#listing-program_2","text":"def seidel ( a , x , b ): #Mencari Panjang Matrix n = len ( a ) for j in range ( 0 , n ): d = b [ j ] #Menghitung xi, yi, zi for i in range ( 0 , n ): if ( j != i ): d -= a [ j ][ i ] * x [ i ] x [ j ] = d / a [ j ][ j ] #Solusi return x m = int ( input ( \"Masukkan Panjang Matrix: \" )) a = [] b = [] for k in range ( m ): mat1 = [] for i in range ( m ): l = float ( input ( \"Masukkan a\" + str ( k + 1 ) + \",\" + str ( i + 1 ) + \": \" )) mat1 . append ( l ) h = float ( input ( \"Masukkan Hasil: \" )) b . append ( h ) a . append ( mat1 ) n = 3 x = [ 0 , 0 , 0 ] print ( x ) for i in range ( 0 , 100 ): x = seidel ( a , x , b ) print ( x ) Output: Masukkan Panjang Matrix: 3 Masukkan a1,1: 4 Masukkan a1,2: -1 Masukkan a1,3: 1 Masukkan Hasil: 7 Masukkan a2,1: 4 Masukkan a2,2: -8 Masukkan a2,3: 1 Masukkan Hasil: -21 Masukkan a3,1: -2 Masukkan a3,2: 1 Masukkan a3,3: 5 Masukkan Hasil: 15 [0, 0, 0] [1.75, 3.5, 3.0] [1.875, 3.9375, 2.9625] [1.99375, 3.9921875, 2.9990625] [1.99828125, 3.9990234375, 2.9995078125] [1.99987890625, 3.9998779296875, 2.9999759765625003] [1.99997548828125, 3.9999847412109375, 2.999993247070312] [1.9999978735351562, 3.9999980926513667, 2.999999530883789] [1.9999996404418945, 3.9999997615814205, 2.9999999038604734] [1.9999999644302369, 3.9999999701976776, 2.9999999917325595] [1.9999999946162794, 3.9999999962747097, 2.99999999859157] [1.9999999994207849, 3.9999999995343387, 2.9999999998614464] [1.9999999999182232, 3.999999999941793, 2.999999999978931] [1.9999999999907154, 3.999999999992724, 2.9999999999977414] [1.9999999999987457, 3.9999999999990905, 2.9999999999996803] [1.9999999999998526, 3.9999999999998863, 2.9999999999999636] [1.9999999999999807, 3.999999999999986, 2.999999999999995] [1.9999999999999978, 3.9999999999999987, 2.9999999999999996] [1.9999999999999996, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] [2.0, 4.0, 3.0] Dari soal diatas persamaan yang dipilih adalah 4x-y+z=7, 4x-8y+z=-21 dan -2x+y+5z=15. Iterasi yang digunakan sebanyak 100 iterasi sehingga dapat menghasilkan x=2,y=4 dan z=3. Sekian dan Terima Kasih","title":"Listing Program"},{"location":"Error%20in%20numerical%20computation/","text":"Error in numerical computation \u00b6 ERROR \u00b6 \u200b Error merupakan perbedaan antara hasil akhir dari sebuah penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah berdampak penting dan bisa dikatakan daruratc, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi. NILAI ERROR \u00b6 \u200b Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif . Absolut error \u00b6 \u200b Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan $$ Ea = |X - x| $$ kesalahan Relatif \u00b6 \u200b Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. $$ Er = |Xt - Xa / Xt| $$ PENYEBAB TERJADINYA ERROR \u00b6 Dibedakan dalam beberapa macam : 1.Round-off-errors \u00b6 \u200b Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja. 2.Truncation errors \u00b6 Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi. True Error Didefinisikan sebagai beda antara nilai eksak dalam penghitungan dan pendekatan menggunakan metode numerik. 3.Inherent error \u00b6 DEFINISI MACLAURIN \u00b6 \u200b Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi. Berikut algoritma dari maclaurin \u00b6 Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi Listing Program \u00b6 membuat program supaya dapaat mengekspansi bilangan e^3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output: masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Error in numerical computation"},{"location":"Error%20in%20numerical%20computation/#error-in-numerical-computation","text":"","title":"Error in numerical computation"},{"location":"Error%20in%20numerical%20computation/#error","text":"\u200b Error merupakan perbedaan antara hasil akhir dari sebuah penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah berdampak penting dan bisa dikatakan daruratc, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi.","title":"ERROR"},{"location":"Error%20in%20numerical%20computation/#nilai-error","text":"\u200b Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif .","title":"NILAI ERROR"},{"location":"Error%20in%20numerical%20computation/#absolut-error","text":"\u200b Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan $$ Ea = |X - x| $$","title":"Absolut error"},{"location":"Error%20in%20numerical%20computation/#kesalahan-relatif","text":"\u200b Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. $$ Er = |Xt - Xa / Xt| $$","title":"kesalahan Relatif"},{"location":"Error%20in%20numerical%20computation/#penyebab-terjadinya-error","text":"Dibedakan dalam beberapa macam :","title":"PENYEBAB TERJADINYA ERROR"},{"location":"Error%20in%20numerical%20computation/#1round-off-errors","text":"\u200b Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja.","title":"1.Round-off-errors"},{"location":"Error%20in%20numerical%20computation/#2truncation-errors","text":"Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi. True Error Didefinisikan sebagai beda antara nilai eksak dalam penghitungan dan pendekatan menggunakan metode numerik.","title":"2.Truncation errors"},{"location":"Error%20in%20numerical%20computation/#3inherent-error","text":"","title":"3.Inherent error"},{"location":"Error%20in%20numerical%20computation/#definisi-maclaurin","text":"\u200b Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi.","title":"DEFINISI MACLAURIN"},{"location":"Error%20in%20numerical%20computation/#berikut-algoritma-dari-maclaurin","text":"Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi","title":"Berikut algoritma dari maclaurin"},{"location":"Error%20in%20numerical%20computation/#listing-program","text":"membuat program supaya dapaat mengekspansi bilangan e^3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output: masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Listing Program"},{"location":"Home/","text":"Welcome Di Halaman Tugas Komputasi Numerik \u00b6 Biodata \u00b6 `Nama : Risky Sulistiyani. `NIM : 180411100036. `Kelas : Komputasi Numerik 4B. `Angkatan : 2018. Dosen Pengampu : Mula'ab,S.si.,M.Kom Alamat : Jakarta","title":"Home"},{"location":"Home/#welcome-di-halaman-tugas-komputasi-numerik","text":"","title":"Welcome Di Halaman Tugas Komputasi Numerik"},{"location":"Home/#biodata","text":"`Nama : Risky Sulistiyani. `NIM : 180411100036. `Kelas : Komputasi Numerik 4B. `Angkatan : 2018. Dosen Pengampu : Mula'ab,S.si.,M.Kom Alamat : Jakarta","title":"Biodata"},{"location":"Metode%20Romberg/","text":"METODE INTEGRASI ROMBERG \u00b6 A. Materi \u00b6 \u200b Integrasi Romberg merupakan teknik yang digunakan dalam integrasi numerik untuk Integrasi Romberg merupakan teknik yang digunakan dalam integrasi numerik untuk menganalisis kasus dimana fungsi yang akan diintegrasikan tersedia. Teknik ini memiliki menganalisis kasus dimana fungsi yang akan diintegrasikan tersedia. Teknik ini memiliki keunggulan untuk menghasilkan nilai-nilai dari fungsi yang digunakan untuk mengembangkan keunggulan untuk menghasilkan nilai-nilai dari fungsi yang digunakan untuk mengembangkan skema yang efisien bagi pengintegrasian secara numerik. Integrasi Romberg didasarkan pada skema yang efisien bagi pengintegrasian secara numerik. Integrasi Romberg didasarkan pada ekstrapolasi Richardson ( ekstrapolasi Richardson ( Richardso Richardson\u2019s n\u2019s extrapol extrapolation ation), yaitu metode untuk mengkombinasikan ), yaitu metode untuk mengkombinasikan dua perkiraan integral secara numerik untuk memperoleh nilai ketiga, yang lebih akurat. \u200b Teknik ini bersifat rekursif dan dapat digunakan untuk menghasilkan sebuah perkiraan Teknik ini bersifat rekursif dan dapat digunakan untuk menghasilkan sebuah perkiraan integral dalam batas toleransi kesalahan ( integral dalam batas toleransi kesalahan (error tolerance error tolerance) yang sudah ditentukan terlebih dahulu. ) yang sudah ditentukan terlebih dahulu. Metode ini digunakan untuk memperbaiki hasil pendekatan integrasi metode trapesium, karena Metode ini digunakan untuk memperbaiki hasil pendekatan integrasi metode trapesium, karena kesalahan metode t kesalahan metode trapesium \u201ccukup\u201d besar rapesium \u201ccukup\u201d besar untuk polinom untuk polinom pangkat tinggi pangkat tinggidan fungsi transeden. \u200b Pada proses integrasi Romberg, mula-mula kita hitung kuadratur dengan lebar langkah h Pada proses integrasi Romberg, mula-mula kita hitung kuadratur dengan lebar langkah h dan 2h. Defisini kuadratur adalah Untuk menurunkan galat hampiran integral dari O(h2) menjadi O(h2n + 2) dapat digunakan ekstrapolasi Richardson seperti dinyatakan dalam teorema : \u200b jika didefinisikan barisan kuadratur {I(i,j) : i >= j dimana j =1, 2, 3, ....} untuk hampiran integral f(x) pada [a, b] sebagai: \u200b I (i, 1) = Ti \u2013 1. i \u2265 1 ( barisan aturan trapezium majemuk) \u200b I (i, 2) = Si \u2013 1. i \u2265 2 (barisan aturan Simpson majemuk) \u200b I (i, 3) = Bi \u2013 1. i \u2265 3 (barisan aturan Boole majemuk) Maka integrasi romberg untuk meningkatkan keakuratan hampiran integral dapat di tulis sebagai B. Code Program \u00b6 import numpy as np def trapezcomp ( f , a , b , n ): \"\"\" Composite trapezoidal function integration INPUTS: f: the function to integrate a: lower bound of integration b: upper bound n: number of panels to create between ``a`` and ``b`` \"\"\" # Initialization h = ( b - a ) / n x = a # Composite rule In = f ( a ) for k in range ( 1 , n ): x = x + h In += 2 * f ( x ) return ( In + f ( b )) * h * 0.5 def romberg ( f , a , b , p ): \"\"\" Romberg integration INPUTS: f: the function to integrate a: lower bound of integration b: upper bound p: number of rows in the Romberg table \"\"\" I = np . zeros (( p , p )) for k in range ( 0 , p ): # Composite trapezoidal rule for 2^k panels I [ k , 0 ] = trapezcomp ( f , a , b , 2 ** k ) # Romberg recursive formula for j in range ( 0 , k ): I [ k , j + 1 ] = ( 4 ** ( j + 1 ) * I [ k , j ] - I [ k - 1 , j ]) / ( 4 ** ( j + 1 ) - 1 ) print ( I [ k , 0 : k + 1 ]) # display intermediate results return I if __name__ == '__main__' : def func ( x ): return np . sin ( x ) p_rows = 4 I = romberg ( func , 0 , np . pi / 2 , p_rows ) solution = I [ p_rows - 1 , p_rows - 1 ] print ( solution ) # 1.00000000814 C. Output \u00b6 Microsoft Windows [ Version 10.0 . 18362.657 ] ( c ) 2019 Microsoft Corporation . All rights reserved . C : \\ Kuliah \\ KomputasiNumerik > python romberg . py [ 0.78539816 ] [ 0.94805945 1.00227988 ] [ 0.9871158 1.00013458 0.99999157 ] [ 0.99678517 1.0000083 0.99999988 1.00000001 ] 1.0000000081440203","title":"Metode Romberg"},{"location":"Metode%20Romberg/#metode-integrasi-romberg","text":"","title":"METODE INTEGRASI ROMBERG"},{"location":"Metode%20Romberg/#a-materi","text":"\u200b Integrasi Romberg merupakan teknik yang digunakan dalam integrasi numerik untuk Integrasi Romberg merupakan teknik yang digunakan dalam integrasi numerik untuk menganalisis kasus dimana fungsi yang akan diintegrasikan tersedia. Teknik ini memiliki menganalisis kasus dimana fungsi yang akan diintegrasikan tersedia. Teknik ini memiliki keunggulan untuk menghasilkan nilai-nilai dari fungsi yang digunakan untuk mengembangkan keunggulan untuk menghasilkan nilai-nilai dari fungsi yang digunakan untuk mengembangkan skema yang efisien bagi pengintegrasian secara numerik. Integrasi Romberg didasarkan pada skema yang efisien bagi pengintegrasian secara numerik. Integrasi Romberg didasarkan pada ekstrapolasi Richardson ( ekstrapolasi Richardson ( Richardso Richardson\u2019s n\u2019s extrapol extrapolation ation), yaitu metode untuk mengkombinasikan ), yaitu metode untuk mengkombinasikan dua perkiraan integral secara numerik untuk memperoleh nilai ketiga, yang lebih akurat. \u200b Teknik ini bersifat rekursif dan dapat digunakan untuk menghasilkan sebuah perkiraan Teknik ini bersifat rekursif dan dapat digunakan untuk menghasilkan sebuah perkiraan integral dalam batas toleransi kesalahan ( integral dalam batas toleransi kesalahan (error tolerance error tolerance) yang sudah ditentukan terlebih dahulu. ) yang sudah ditentukan terlebih dahulu. Metode ini digunakan untuk memperbaiki hasil pendekatan integrasi metode trapesium, karena Metode ini digunakan untuk memperbaiki hasil pendekatan integrasi metode trapesium, karena kesalahan metode t kesalahan metode trapesium \u201ccukup\u201d besar rapesium \u201ccukup\u201d besar untuk polinom untuk polinom pangkat tinggi pangkat tinggidan fungsi transeden. \u200b Pada proses integrasi Romberg, mula-mula kita hitung kuadratur dengan lebar langkah h Pada proses integrasi Romberg, mula-mula kita hitung kuadratur dengan lebar langkah h dan 2h. Defisini kuadratur adalah Untuk menurunkan galat hampiran integral dari O(h2) menjadi O(h2n + 2) dapat digunakan ekstrapolasi Richardson seperti dinyatakan dalam teorema : \u200b jika didefinisikan barisan kuadratur {I(i,j) : i >= j dimana j =1, 2, 3, ....} untuk hampiran integral f(x) pada [a, b] sebagai: \u200b I (i, 1) = Ti \u2013 1. i \u2265 1 ( barisan aturan trapezium majemuk) \u200b I (i, 2) = Si \u2013 1. i \u2265 2 (barisan aturan Simpson majemuk) \u200b I (i, 3) = Bi \u2013 1. i \u2265 3 (barisan aturan Boole majemuk) Maka integrasi romberg untuk meningkatkan keakuratan hampiran integral dapat di tulis sebagai","title":"A. Materi"},{"location":"Metode%20Romberg/#b-code-program","text":"import numpy as np def trapezcomp ( f , a , b , n ): \"\"\" Composite trapezoidal function integration INPUTS: f: the function to integrate a: lower bound of integration b: upper bound n: number of panels to create between ``a`` and ``b`` \"\"\" # Initialization h = ( b - a ) / n x = a # Composite rule In = f ( a ) for k in range ( 1 , n ): x = x + h In += 2 * f ( x ) return ( In + f ( b )) * h * 0.5 def romberg ( f , a , b , p ): \"\"\" Romberg integration INPUTS: f: the function to integrate a: lower bound of integration b: upper bound p: number of rows in the Romberg table \"\"\" I = np . zeros (( p , p )) for k in range ( 0 , p ): # Composite trapezoidal rule for 2^k panels I [ k , 0 ] = trapezcomp ( f , a , b , 2 ** k ) # Romberg recursive formula for j in range ( 0 , k ): I [ k , j + 1 ] = ( 4 ** ( j + 1 ) * I [ k , j ] - I [ k - 1 , j ]) / ( 4 ** ( j + 1 ) - 1 ) print ( I [ k , 0 : k + 1 ]) # display intermediate results return I if __name__ == '__main__' : def func ( x ): return np . sin ( x ) p_rows = 4 I = romberg ( func , 0 , np . pi / 2 , p_rows ) solution = I [ p_rows - 1 , p_rows - 1 ] print ( solution ) # 1.00000000814","title":"B. Code Program"},{"location":"Metode%20Romberg/#c-output","text":"Microsoft Windows [ Version 10.0 . 18362.657 ] ( c ) 2019 Microsoft Corporation . All rights reserved . C : \\ Kuliah \\ KomputasiNumerik > python romberg . py [ 0.78539816 ] [ 0.94805945 1.00227988 ] [ 0.9871158 1.00013458 0.99999157 ] [ 0.99678517 1.0000083 0.99999988 1.00000001 ] 1.0000000081440203","title":"C. Output"},{"location":"Numerical%20Solution/","text":"Numerical Solution of Algebraic and Transcendental Equation \u00b6 Ekspresi bentuk f x = a0xn + a1xn \u2212 1 + \u22ef + an \u2212 1x + an, a0 \u2260 0 disebut polinomial derajat ' n'and polinomial f x = 0 disebut persamaan aljabar dari derajat ke-n. Jika f x mengandung fungsi trigonometri, logaritmik atau eksponensial, maka f x = 0 disebut persamaan transendental. Untuk examplex2 + 2sinx + ex = 0 adalah persamaan transendental. Jika f x adalah polinomial aljabar derajat kurang dari atau sama dengan 4, metode langsung untuk menemukan akar persamaan tersebut tersedia. Tetapi jika f x adalah tingkat yang lebih tinggi atau melibatkan fungsi Transendental, metode langsung tidak ada dan kita perlu menerapkan metode numerik untuk menemukan akar dari persamaan f x = 0. Persamaan transcendental : sin, cos, tan, Persamaan polynomial : a0 + a1x + a2x2 \u2026\u2026. + anxn + \u2026\u2026 Penyelesaian persamaan non linier \u00b6 Metode Tertutup Mencari akar pada range [a,b] tertentu Dalam range[a,b] dipastikan terdapat satu akar Hasil selalu konvergen \u2192 disebut juga metode konvergen Contohnya Metode Tabel ,Metode Biseksi,Metode Regula Falsi Metode Terbuka Diperlukan tebakan awal xn dipakai untuk menghitung xn+1 Hasil dapat konvergen atau divergen Contohnya Metode Iterasi Sederhana, Metode Newton-Raphson, Metode Secant. method Bisection Untuk menggunakan metode bisection, terlebih dahulu ditentukan batas bawah (a) dan batas atas (b). Kemudian dihitung nilai tengah: Dari nilai x ini perlu dilakukan pengecekan keberadaan akar. Secara matematik, suatu range terdapat akar persamaan bila f(a) dan f(b) berlawanan tanda. Setelah diketahui dibagian mana yang terdapat akar, maka batas bawah dan batas atas di perbaharui sesuai dengan range dari bagian yang mempunyai akar. Batasan a dan b memberikan harga bagi fungsi f(x) untuk x = a dan x = b. Langkah selanjutnya adalah memeriksa apakah f(a) \u00d7 f(b) < 0. Metode biseksi ini membagi range menjadi 2 bagian, dari dua bagian ini dipilih bagian mana yang mengandung akar sedangkan bagian yang tidak mengandung akar akan dibuang. Hal ini dilakukan berulang-ulang hingga diperoleh suatu akar persamaan langkah langkah Tentukan batas bawah (a) dan batas atas (b). Kemudian dihitung nilai tengah : $$ c = {(a+b)\\over 2} $$ Dari nilai c ini perlu dilakukan pengecekan keberadaan akar. Secara matematik, suatu range terdapat akar persamaan bila f(a) dan f(b) berlawanan tanda atau dituliskan : $$ f(a).f(b) <0 $$ Setelah diketahui di bagian mana terdapat akar, maka batas bawah dan batas atas diperbarui sesuai dengan range dari bagian yang mempunyai akar algoritma metode bisection 1.Definisikan fungsi f(x) yang akan dicari akarnya 2.Tentukan nilai a dan b 3.Tentukan toleransi e dan iterasi maksimum N 4.Hitung f(a) dan f(b) 5.Jika f(a).f(b)>0 maka proses dihentikan karena tidak ada akar, bila tidak maka dilanjutkan 6.Hitung x = (a+b)/2 7.Hitung f(x) 8.Bila f(x).f(a)<0 maka b = x dan f(b)=f(x), bila tidak maka a=x dan f(a)=f(x) 9.Jika |b-a|< e atau iterasi > iterasi maks maka proses dihentikan dan didapatkan akar x, bila tidak, ulangi langkah 6 implementasi metode bisection dalam python \u00b6 def bisection ( f , a , b , N ): if f ( a ) * f ( b ) >= 0 : print ( \"Bisection method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = ( a_n + b_n ) / 2 f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Bisection method fails.\" ) return None return ( a_n + b_n ) / 2 f = lambda x : x ** 2 - 5 * x + 6 approx_phi = bisection ( f , 1 , 2.3 , 25 ) print ( approx_phi ) 1.9999999985098835 methot Regula Falsi 1.Metode pencarian akar persamaan dengan memanfaatkan kemiringan dan selisih tinggi dari dua titik batas range. 2.Dua titik a dan b pada fungsi f(x) digunakan untuk mengestimasi posisi c dari akar interpolasi linier. 3.Dikenal dengan metode False Position 4.Metode ini juga merupakan penyempurna dari metode bisection algoritma method regula falsi Definisikan fungsi f(x) Tentukan batas bawah (a) dan batas atas (b) Tentukan toleransi error e Hitung f(a) dan f(b) Untuk iterasi 1 s/d n > e : $$ c= {(f(b).a -f(a).b)\\over (f(b)-f(a))} $$ Hitung f(c)=f(x) \u200b Hitung error = |f(c)| \u200b Jika f(c).f(a)<0 maka nilai a tetap ,jika tidak maka a=c dan f(a)=f(c) Akar persamaanya = c implementasi method regula falsi dalam python \u00b6 error = 0.01 a = 0 b = 2.1 def f ( x ): return x ** 2 - 5 * x + 6 def regulasi_falsi ( a , b ): i = 0 max_iter = 50 iteration = True while iteration and i < max_iter : if f ( a ) * f ( b ) < 0 : x = ( a * abs ( f ( b )) + b * abs ( f ( a ))) / ( abs ( f ( a )) + abs ( f ( b ))) if f ( a ) * f ( x ) < 0 : b = x if f ( x ) * f ( b ) < 0 : a = x if abs ( a - b ) < error : iteration = False else : i += 1 else : print ( 'tidak di temukan akar' ) print ( 'x =' , x ) regulasi_falsi ( a , b ) x = 2.000000000174259 Metode Newton Raphson Metode Newton Raphson biasa digunakan dalam mencari akar dari suatu persamaan non linier, jika diasumsikan f mempunyai turunan kontinu f\u2019. Metode Newton Rapshon sering digunakan karena kesederhanaannya dan mempunyai konvergensi yang cepat. Karena metode ini merupakan metode Terbuka, maka tetap diperlukan nilai tebakan awal untuk Xo Metode pencarian akar persamaan dengan memanfaatkan kemiringan dan selisih tinggi dari dua titik batas range. Dua titik a dan b pada fungsi f(x) digunakan untuk mengestimasi posisi c dari akar interpolasi linier. Dikenal dengan metode False Position Metode ini juga merupakan penyempurna dari metode bisection Prinsip Metode Newton Raphson Metode Newton-Raphson adalah metode pencarian akar suatu fungsi f(x) dengan pendekatan satu titik, dimana fungsi f(x) mempunyai turunan. Metode ini dianggap lebih mudah dari Metode Bagi-Dua (Bisection Method) karena metode ini menggunakan pendekatan satu titik sebagai titik awal. prosedur Menentukan x0 sebagai titik awal. Menarik garis lurus (misal garis P) yang menyinggung titik f(x0). Hal ini berakibat garis P memotong sumbu-x di titik x1. Ulangi langkah sebelumnya tapi sekarang x1 dianggap sebagai titik awalnya. Dari mengulang langkah-langkah sebelumnya akan mendapatkan x1,x2,x3,...,,xn dengan xn yang diperoleh adalah bilangan riil yang merupakan akar atau mendekati akar yang sebenarnya implementasi Metode Newton Raphson python \u00b6 def newton ( f , Df , x0 , epsilon , max_iter ): xn = x0 for n in range ( 0 , max_iter ): fxn = f ( xn ) if abs ( fxn ) < epsilon : print ( 'Found solution after' , n , 'iterations.' ) return xn Dfxn = Df ( xn ) if Dfxn == 0 : print ( 'Zero derivative. No solution found.' ) return None xn = xn - fxn / Dfxn print ( 'Exceeded maximum iterations. No solution found.' ) return None p = lambda x : x ** 2 - 5 * x + 6 Dp = lambda x : 2 * x - 5 approx = newton ( p , Dp , 1 , 1e-3 , 10 ) print ( approx ) Found solution after 4 iterations . 1.9999847409781035 Metode Secant \u25a0Metode Newton Raphson memerlukan perhitungan turunan fungsi f\u2019(x). \u25a0Tidak semua fungsi mudah dicari turunannya terutama fungsi yang bentuknya rumit. \u25a0Turunan fungsi dapat dihilangkan dengan cara menggantinya dengan bentuk lain yang ekivalen \u25a0Modifikasi metode Newton Raphson dinamakan metode Secant. formula secant \u00b6 $$ y = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ $$ 0 = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ $$ x = a - f(a)\\frac{b - a}{f(b) - f(a)} $$ algoritma method secant \u00b6 \u25a0 Definisikan f(x) \u25a0 Definisikan toleransi error e dan iterasi maksimum (n) \u25a0 Masukan dua nilai pendekatan awal yang diantaranya terdapat akar yaitu x_0 dan x_1 ,sebaiknya gunakan metode tabel untuk menjamin titik pendekatanya adalah titik pendekatan yang konvergensinya pada akar persamaan yang diharapkan. \u25a0 Hitung f(x_0 ) dan fx_1 sebagai y_0 dan y_1 \u25a0 Untuk iterasi 1 s/d n x_(i+1)= x_i-(f(xi)(x_i \u3016-x\u3017 (i-1)))/(y_i - y (i-1) ) hitung y_(i+1)=\u3016f(x\u3017_(i+1)) Akar persamaan adalah nilai x yang terakhir implementasi method secant pada python \u00b6 def secant ( f , a , b , N ): if f ( a ) * f ( b ) >= 0 : print ( \"Secant method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Secant method fails.\" ) return None return a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) p = lambda x : x ** 2 - 5 * x + 6 approx = secant ( p , 1 , 2.4 , 20 ) print ( approx ) 2.0000003178913373 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Numerical Solution"},{"location":"Numerical%20Solution/#numerical-solution-of-algebraic-and-transcendental-equation","text":"Ekspresi bentuk f x = a0xn + a1xn \u2212 1 + \u22ef + an \u2212 1x + an, a0 \u2260 0 disebut polinomial derajat ' n'and polinomial f x = 0 disebut persamaan aljabar dari derajat ke-n. Jika f x mengandung fungsi trigonometri, logaritmik atau eksponensial, maka f x = 0 disebut persamaan transendental. Untuk examplex2 + 2sinx + ex = 0 adalah persamaan transendental. Jika f x adalah polinomial aljabar derajat kurang dari atau sama dengan 4, metode langsung untuk menemukan akar persamaan tersebut tersedia. Tetapi jika f x adalah tingkat yang lebih tinggi atau melibatkan fungsi Transendental, metode langsung tidak ada dan kita perlu menerapkan metode numerik untuk menemukan akar dari persamaan f x = 0. Persamaan transcendental : sin, cos, tan, Persamaan polynomial : a0 + a1x + a2x2 \u2026\u2026. + anxn + \u2026\u2026","title":"Numerical Solution of Algebraic and Transcendental Equation"},{"location":"Numerical%20Solution/#penyelesaian-persamaan-non-linier","text":"Metode Tertutup Mencari akar pada range [a,b] tertentu Dalam range[a,b] dipastikan terdapat satu akar Hasil selalu konvergen \u2192 disebut juga metode konvergen Contohnya Metode Tabel ,Metode Biseksi,Metode Regula Falsi Metode Terbuka Diperlukan tebakan awal xn dipakai untuk menghitung xn+1 Hasil dapat konvergen atau divergen Contohnya Metode Iterasi Sederhana, Metode Newton-Raphson, Metode Secant. method Bisection Untuk menggunakan metode bisection, terlebih dahulu ditentukan batas bawah (a) dan batas atas (b). Kemudian dihitung nilai tengah: Dari nilai x ini perlu dilakukan pengecekan keberadaan akar. Secara matematik, suatu range terdapat akar persamaan bila f(a) dan f(b) berlawanan tanda. Setelah diketahui dibagian mana yang terdapat akar, maka batas bawah dan batas atas di perbaharui sesuai dengan range dari bagian yang mempunyai akar. Batasan a dan b memberikan harga bagi fungsi f(x) untuk x = a dan x = b. Langkah selanjutnya adalah memeriksa apakah f(a) \u00d7 f(b) < 0. Metode biseksi ini membagi range menjadi 2 bagian, dari dua bagian ini dipilih bagian mana yang mengandung akar sedangkan bagian yang tidak mengandung akar akan dibuang. Hal ini dilakukan berulang-ulang hingga diperoleh suatu akar persamaan langkah langkah Tentukan batas bawah (a) dan batas atas (b). Kemudian dihitung nilai tengah : $$ c = {(a+b)\\over 2} $$ Dari nilai c ini perlu dilakukan pengecekan keberadaan akar. Secara matematik, suatu range terdapat akar persamaan bila f(a) dan f(b) berlawanan tanda atau dituliskan : $$ f(a).f(b) <0 $$ Setelah diketahui di bagian mana terdapat akar, maka batas bawah dan batas atas diperbarui sesuai dengan range dari bagian yang mempunyai akar algoritma metode bisection 1.Definisikan fungsi f(x) yang akan dicari akarnya 2.Tentukan nilai a dan b 3.Tentukan toleransi e dan iterasi maksimum N 4.Hitung f(a) dan f(b) 5.Jika f(a).f(b)>0 maka proses dihentikan karena tidak ada akar, bila tidak maka dilanjutkan 6.Hitung x = (a+b)/2 7.Hitung f(x) 8.Bila f(x).f(a)<0 maka b = x dan f(b)=f(x), bila tidak maka a=x dan f(a)=f(x) 9.Jika |b-a|< e atau iterasi > iterasi maks maka proses dihentikan dan didapatkan akar x, bila tidak, ulangi langkah 6","title":"Penyelesaian persamaan non linier"},{"location":"Numerical%20Solution/#implementasi-metode-bisection-dalam-python","text":"def bisection ( f , a , b , N ): if f ( a ) * f ( b ) >= 0 : print ( \"Bisection method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = ( a_n + b_n ) / 2 f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Bisection method fails.\" ) return None return ( a_n + b_n ) / 2 f = lambda x : x ** 2 - 5 * x + 6 approx_phi = bisection ( f , 1 , 2.3 , 25 ) print ( approx_phi ) 1.9999999985098835 methot Regula Falsi 1.Metode pencarian akar persamaan dengan memanfaatkan kemiringan dan selisih tinggi dari dua titik batas range. 2.Dua titik a dan b pada fungsi f(x) digunakan untuk mengestimasi posisi c dari akar interpolasi linier. 3.Dikenal dengan metode False Position 4.Metode ini juga merupakan penyempurna dari metode bisection algoritma method regula falsi Definisikan fungsi f(x) Tentukan batas bawah (a) dan batas atas (b) Tentukan toleransi error e Hitung f(a) dan f(b) Untuk iterasi 1 s/d n > e : $$ c= {(f(b).a -f(a).b)\\over (f(b)-f(a))} $$ Hitung f(c)=f(x) \u200b Hitung error = |f(c)| \u200b Jika f(c).f(a)<0 maka nilai a tetap ,jika tidak maka a=c dan f(a)=f(c) Akar persamaanya = c","title":"implementasi metode bisection dalam python"},{"location":"Numerical%20Solution/#implementasi-method-regula-falsi-dalam-python","text":"error = 0.01 a = 0 b = 2.1 def f ( x ): return x ** 2 - 5 * x + 6 def regulasi_falsi ( a , b ): i = 0 max_iter = 50 iteration = True while iteration and i < max_iter : if f ( a ) * f ( b ) < 0 : x = ( a * abs ( f ( b )) + b * abs ( f ( a ))) / ( abs ( f ( a )) + abs ( f ( b ))) if f ( a ) * f ( x ) < 0 : b = x if f ( x ) * f ( b ) < 0 : a = x if abs ( a - b ) < error : iteration = False else : i += 1 else : print ( 'tidak di temukan akar' ) print ( 'x =' , x ) regulasi_falsi ( a , b ) x = 2.000000000174259 Metode Newton Raphson Metode Newton Raphson biasa digunakan dalam mencari akar dari suatu persamaan non linier, jika diasumsikan f mempunyai turunan kontinu f\u2019. Metode Newton Rapshon sering digunakan karena kesederhanaannya dan mempunyai konvergensi yang cepat. Karena metode ini merupakan metode Terbuka, maka tetap diperlukan nilai tebakan awal untuk Xo Metode pencarian akar persamaan dengan memanfaatkan kemiringan dan selisih tinggi dari dua titik batas range. Dua titik a dan b pada fungsi f(x) digunakan untuk mengestimasi posisi c dari akar interpolasi linier. Dikenal dengan metode False Position Metode ini juga merupakan penyempurna dari metode bisection Prinsip Metode Newton Raphson Metode Newton-Raphson adalah metode pencarian akar suatu fungsi f(x) dengan pendekatan satu titik, dimana fungsi f(x) mempunyai turunan. Metode ini dianggap lebih mudah dari Metode Bagi-Dua (Bisection Method) karena metode ini menggunakan pendekatan satu titik sebagai titik awal. prosedur Menentukan x0 sebagai titik awal. Menarik garis lurus (misal garis P) yang menyinggung titik f(x0). Hal ini berakibat garis P memotong sumbu-x di titik x1. Ulangi langkah sebelumnya tapi sekarang x1 dianggap sebagai titik awalnya. Dari mengulang langkah-langkah sebelumnya akan mendapatkan x1,x2,x3,...,,xn dengan xn yang diperoleh adalah bilangan riil yang merupakan akar atau mendekati akar yang sebenarnya","title":"implementasi method regula falsi dalam python"},{"location":"Numerical%20Solution/#implementasi-metode-newton-raphson-python","text":"def newton ( f , Df , x0 , epsilon , max_iter ): xn = x0 for n in range ( 0 , max_iter ): fxn = f ( xn ) if abs ( fxn ) < epsilon : print ( 'Found solution after' , n , 'iterations.' ) return xn Dfxn = Df ( xn ) if Dfxn == 0 : print ( 'Zero derivative. No solution found.' ) return None xn = xn - fxn / Dfxn print ( 'Exceeded maximum iterations. No solution found.' ) return None p = lambda x : x ** 2 - 5 * x + 6 Dp = lambda x : 2 * x - 5 approx = newton ( p , Dp , 1 , 1e-3 , 10 ) print ( approx ) Found solution after 4 iterations . 1.9999847409781035 Metode Secant \u25a0Metode Newton Raphson memerlukan perhitungan turunan fungsi f\u2019(x). \u25a0Tidak semua fungsi mudah dicari turunannya terutama fungsi yang bentuknya rumit. \u25a0Turunan fungsi dapat dihilangkan dengan cara menggantinya dengan bentuk lain yang ekivalen \u25a0Modifikasi metode Newton Raphson dinamakan metode Secant.","title":"implementasi Metode Newton Raphson python"},{"location":"Numerical%20Solution/#formula-secant","text":"$$ y = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ $$ 0 = \\frac{f(b) - f(a)}{b - a}(x - a) + f(a) $$ $$ x = a - f(a)\\frac{b - a}{f(b) - f(a)} $$","title":"formula secant"},{"location":"Numerical%20Solution/#algoritma-method-secant","text":"\u25a0 Definisikan f(x) \u25a0 Definisikan toleransi error e dan iterasi maksimum (n) \u25a0 Masukan dua nilai pendekatan awal yang diantaranya terdapat akar yaitu x_0 dan x_1 ,sebaiknya gunakan metode tabel untuk menjamin titik pendekatanya adalah titik pendekatan yang konvergensinya pada akar persamaan yang diharapkan. \u25a0 Hitung f(x_0 ) dan fx_1 sebagai y_0 dan y_1 \u25a0 Untuk iterasi 1 s/d n x_(i+1)= x_i-(f(xi)(x_i \u3016-x\u3017 (i-1)))/(y_i - y (i-1) ) hitung y_(i+1)=\u3016f(x\u3017_(i+1)) Akar persamaan adalah nilai x yang terakhir","title":"algoritma method secant"},{"location":"Numerical%20Solution/#implementasi-method-secant-pada-python","text":"def secant ( f , a , b , N ): if f ( a ) * f ( b ) >= 0 : print ( \"Secant method fails.\" ) return None a_n = a b_n = b for n in range ( 1 , N + 1 ): m_n = a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) f_m_n = f ( m_n ) if f ( a_n ) * f_m_n < 0 : a_n = a_n b_n = m_n elif f ( b_n ) * f_m_n < 0 : a_n = m_n b_n = b_n elif f_m_n == 0 : print ( \"Found exact solution.\" ) return m_n else : print ( \"Secant method fails.\" ) return None return a_n - f ( a_n ) * ( b_n - a_n ) / ( f ( b_n ) - f ( a_n )) p = lambda x : x ** 2 - 5 * x + 6 approx = secant ( p , 1 , 2.4 , 20 ) print ( approx ) 2.0000003178913373 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"implementasi method secant pada python"},{"location":"Richardson%20Extrapolation/","text":"Richardson Extrapolation \u00b6 Dalam analisis numerik, Richardson Extrapolation adalah metode percepatan urutan, yang digunakan untuk meningkatkan laju konvergensi suatu urutan. Richardson Extrapolation termasuk integrasi Romberg, yang menerapkan ekstrapolasi Richardson pada aturan trapesium, dan algoritma Bulirsch-Stoer untuk menyelesaikan persamaan diferensial biasa. Teori \u00b6 Dalam rumus : ( f (x + h) - f (x - h) ) / (2 h) untuk nilai h yang sangat kecil, dua fungsi evaluasi f (x + h) dan f (x - h) akan menjadi kira-kira sama, dan oleh karena itu pembatalan subtraktif akan terjadi. Oleh karena itu, tidak disarankan untuk menggunakan nilai h yang semakin kecil. Kita dapat mencoba untuk memperkirakan nilai tepat e dengan perkiraan a(h) . Dalam hal ini, e adalah turunan dari f (1) (x) dan perkiraannya adalah ( h ) = (f (x + h) - f (x - h)) / (2 h) . Misalkan sekarang bahwa kesalahan aproksimasi didefinisikan oleh serangkaian bentuk Taylor : e = a(h) + K h n + o(h n ) Apabila menggunakan h / 2 : e = a(h/2) + K (h/2)n + o((h/2)n) = a(h/2) + K/2n h n + o(h n ) Mengalikan kedua ekspresi ini dengan 2 n dan mengurangi hasil persamaan pertama 2n e \u2212 e = 2na(h/2) \u2212 a(h) + K/2n h n \u2212 K h n + o(h n ) Perhatikan bahwa istilah h n dibatalkan dan kita dibiarkan dengan (2n \u2212 1)e = 2na(h/2) \u2212 a(h) + o(h n ) Jika kita melihat seri Taylor lengkap untuk rumus perbedaan-terpusat yang terpusat, kita perhatikan bahwa istilah kesalahannya dalam bentuk Knh n . Dapat kita tulis dengan : K1 = \u22121/6 f(3)(x)h 2 , etc. Contoh Program \u00b6 from math import * def zeros ( n , m ): Z = [] for i in range ( n ): Z . append ([ 0 ] * m ) return Z def D ( Func , a , h ): return ( Func ( a + h ) - Func ( a - h )) / ( 2 * h ) def Richardson_dif ( func , a ): '''Richardson extrapolation method for numerical calculation of first derivative ''' k = 9 L = zeros ( k , k ) for I in range ( k ): L [ I ][ 0 ] = D ( func , a , 1 / ( 2 ** ( I + 1 ))) for j in range ( 1 , k ): for i in range ( k - j ): L [ i ][ j ] = (( 4 ** ( j )) * L [ i + 1 ][ j - 1 ] - L [ i ][ j - 1 ]) / ( 4 ** ( j ) - 1 ) return L [ 0 ][ k - 1 ] print ( '>>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<<' ) print ( \"=======================================================================\" ) print ( 'f = -0.1*x**4-0.15*x**3-0.5*x**2-0.25*x+1.2 dengan x = 0.5' ) print ( \"=======================================================================\" ) print ( ' %04.20f ' % Richardson_dif ( lambda x : - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 , 0.5 )) print ( \"=======================================================================\" ) print ( 'diff(2**cos(pi+sin(x)) dengan x = pi/2 adalah = %04.20f ' % Richardson_dif ( lambda x : 2 ** cos ( pi + sin ( x )), pi / 3 )) Hasil Running \u00b6 >>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<< ======================================================================= f = - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 dengan x = 0.5 ======================================================================= - 0.91250000000000530687 ======================================================================= diff ( 2 ** cos ( pi + sin ( x )) dengan x = pi / 2 adalah = 0.16849558398154249050 >>>","title":"Richardson Extrapolation"},{"location":"Richardson%20Extrapolation/#richardson-extrapolation","text":"Dalam analisis numerik, Richardson Extrapolation adalah metode percepatan urutan, yang digunakan untuk meningkatkan laju konvergensi suatu urutan. Richardson Extrapolation termasuk integrasi Romberg, yang menerapkan ekstrapolasi Richardson pada aturan trapesium, dan algoritma Bulirsch-Stoer untuk menyelesaikan persamaan diferensial biasa.","title":"Richardson Extrapolation"},{"location":"Richardson%20Extrapolation/#teori","text":"Dalam rumus : ( f (x + h) - f (x - h) ) / (2 h) untuk nilai h yang sangat kecil, dua fungsi evaluasi f (x + h) dan f (x - h) akan menjadi kira-kira sama, dan oleh karena itu pembatalan subtraktif akan terjadi. Oleh karena itu, tidak disarankan untuk menggunakan nilai h yang semakin kecil. Kita dapat mencoba untuk memperkirakan nilai tepat e dengan perkiraan a(h) . Dalam hal ini, e adalah turunan dari f (1) (x) dan perkiraannya adalah ( h ) = (f (x + h) - f (x - h)) / (2 h) . Misalkan sekarang bahwa kesalahan aproksimasi didefinisikan oleh serangkaian bentuk Taylor : e = a(h) + K h n + o(h n ) Apabila menggunakan h / 2 : e = a(h/2) + K (h/2)n + o((h/2)n) = a(h/2) + K/2n h n + o(h n ) Mengalikan kedua ekspresi ini dengan 2 n dan mengurangi hasil persamaan pertama 2n e \u2212 e = 2na(h/2) \u2212 a(h) + K/2n h n \u2212 K h n + o(h n ) Perhatikan bahwa istilah h n dibatalkan dan kita dibiarkan dengan (2n \u2212 1)e = 2na(h/2) \u2212 a(h) + o(h n ) Jika kita melihat seri Taylor lengkap untuk rumus perbedaan-terpusat yang terpusat, kita perhatikan bahwa istilah kesalahannya dalam bentuk Knh n . Dapat kita tulis dengan : K1 = \u22121/6 f(3)(x)h 2 , etc.","title":"Teori"},{"location":"Richardson%20Extrapolation/#contoh-program","text":"from math import * def zeros ( n , m ): Z = [] for i in range ( n ): Z . append ([ 0 ] * m ) return Z def D ( Func , a , h ): return ( Func ( a + h ) - Func ( a - h )) / ( 2 * h ) def Richardson_dif ( func , a ): '''Richardson extrapolation method for numerical calculation of first derivative ''' k = 9 L = zeros ( k , k ) for I in range ( k ): L [ I ][ 0 ] = D ( func , a , 1 / ( 2 ** ( I + 1 ))) for j in range ( 1 , k ): for i in range ( k - j ): L [ i ][ j ] = (( 4 ** ( j )) * L [ i + 1 ][ j - 1 ] - L [ i ][ j - 1 ]) / ( 4 ** ( j ) - 1 ) return L [ 0 ][ k - 1 ] print ( '>>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<<' ) print ( \"=======================================================================\" ) print ( 'f = -0.1*x**4-0.15*x**3-0.5*x**2-0.25*x+1.2 dengan x = 0.5' ) print ( \"=======================================================================\" ) print ( ' %04.20f ' % Richardson_dif ( lambda x : - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 , 0.5 )) print ( \"=======================================================================\" ) print ( 'diff(2**cos(pi+sin(x)) dengan x = pi/2 adalah = %04.20f ' % Richardson_dif ( lambda x : 2 ** cos ( pi + sin ( x )), pi / 3 ))","title":"Contoh Program"},{"location":"Richardson%20Extrapolation/#hasil-running","text":">>>>>>>>>>>>>>>>>>>>>>> DIFERENSIASI NUMERIK DARI <<<<<<<<<<<<<<<<<<<<< ======================================================================= f = - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 dengan x = 0.5 ======================================================================= - 0.91250000000000530687 ======================================================================= diff ( 2 ** cos ( pi + sin ( x )) dengan x = pi / 2 adalah = 0.16849558398154249050 >>>","title":"Hasil Running"},{"location":"Tugas%201/","text":"Memahami Data \u00b6 Statistik Deskriptif \u00b6 Statistik deskriptif adalah metode dari pengorganisasian, penjumlahan, dan penyajian data dalam sebuah cara yang nyaman dan informatif, termasuk teknik grafik, dan teknik penghitungan. Pada sesi tulisan ini teknik grafik tidak akan dibahas, penyajian teknik ini akan dijelaskan pada sesi tulisan yang lain. Statistik deskriptif dapat mendeskripsikan data yang sedang dianalisis, tetapi tidak boleh menarik kesimpulan apapun dari data. Untuk pengambilan keputusan, kita perlukan cabang dari ilmu statistik lainnya yaitu statistik inferensia. Statistik inferensia akan dijelaskan pada sesi tulisan yang lain Ukuran-ukuran statistik deskriptif Ukuran statistik deskriptif dapat digolongkan menjadi dua kelompok, yaitu ukuran nilai tengah dan ukuran deviasi. Ukuran nilai tengah terdiri dari rata-rata ( mean ), median, dan modus. Sedangkan ukuran deviasi terdiri dari varians, simpangan baku, koefisien variasi, dan nilai jarak ( range ). Ukuran-ukuran statistik deskriptif tersebut akan dijelaskan penggunaannya baik untuk data tunggal maupun data berkelompok. Ukuran nilai tengah Rata-rata (Mean) Rata-rata ditulis dengan menggunakan simbol \u03bc (dibaca:\u201dmiu\u201d) untuk menyatakan rata-rata populasi, dan (dibaca: x bar) untuk menyatakan rata-rata sampel. Secara aljabar rata-rata dapat ditulis sebagai berikut: \u200b untuk rata-rata populasi \u200b \u200b dimana N adalah banyaknya populasi \u200b untuk rata-rata sampel \u200b \u200b dimana n adalah banyaknya sampel contoh: Dari 11 pohon pear menghasilkan buah dengan berat sbb (dlm Kg): 330 284 326 268 236 346 326 402 374 292 380 Hitunglah rata-rata produksi 11 pohon pear ? Jadi rata-rata produksi dari 11 pohon pear adalah 324 Kg. Rata-rata untuk data berkelompok . Apabila data sudah disajikan dalam data berkelompok seperti dalam bentuk tabel frekuensi dimana observasi-observasi dikelompokan kedalam kelas-kelas yang disebut frekuensi, maka rumus rata-ratanya adalah sebagai berikut: \u200b Contoh: Hitunglah rata-rata nilai statistik dari 50 mahasiswa pada Table 1 dibawah ini. \u200b \u200b Jadi perkiraan rata-rata nilai statistik 50 mahasiswa adalah 65,7. Median Ukuran nilai tengah lainnya yang mungkin dapat merupakan pilihan selain rata-rata adalah median. Jika data pada contoh produksi buah pear diurutkan dari nilai terkecil hingga ke nilai terbesar, maka nilai tengahnya adalah 326 kg artinya lima pohon pear mempunyai produksi dibawahnya dan lima pohon pear mempunyai produksi diatasnya. Nilai tengah inilah yang dikatakan median . Penentuan median bisa langsung didapat jika jumlah observasinya adalah ganjil, namun jika jumlah observasinya adalah genap maka akan didapat dua nilai tengah. Dalam situasi demikian, untuk mendapatkan mediannya yaitu dengan merata-ratakan dua nilai tengah yang didapat. Prosedur untuk mendapatkan median yaitu harus mengurutkan data dari yang terkecil hingga yang terbesar terlebih dahulu sebelum mengambil nilai tengahnya. Dengan kata lain median adalah data yang ke . Median untuk data berkelompok Untuk data yang sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka mediannya dapat dicari dengan rumus sebagai berikut: Kelas median adalah kelas dimana terdapat nilai median di dalamnya. Untuk menentukan kelas median bagilah seluruh observasi dengan dua artinya 50 % dari seluruh observasi terletak sebelum median dan 50 % lainnya terletak sesudahnya. Jika kita lihat tabel frekuensi (Tabel 1) maka mediannya merupakan observasi yang ke (50/2) yaitu yang ke 25. Jumlah tiga frekuensi pertama ( f1 + f2 + f3 ) yaitu 3 + 5 + 8 = 16. Untuk mencapai 25 observasi diperlukan 9 observasi lagi. 9 observasi tersebut dapat dipenuhi dari frekuensi keempat ( f4 ) karena jumlah observasi f4 ada sebanyak 14 observasi. Jadi median terletak pada kelas keempat atau kelas (60 \u2013 69) dengan kata lain kelas keempat adalah kelas median. contoh: Hitunglah nilai median dari data kelompok pada Tabel 1. solusi: \u200b Jadi mediannya, \u200b Pertanyaan yang mungkin timbul adalah jika kita punya data aslinya, apakah nilai median yang sebenarnya adalah 66,33? jawabannya belum tentu, karena cara ini adalah cara interpolasi dimana data aslinya memang tidak diketahui, yang ada adalah data sudah dalam bentuk tabel frekuensi atau sudah dikelompokkan. Walaupun hasil interpolasi ini mungkin tidak tepat, namun cara ini memberikan hasil yang mendekati nilai median yang sebenarnya. Modus Modus dari suatu kelompok observasi adalah nilai observasi yang mempunyai frekuensi pemunculan paling banyak atau dengan kata lain yaitu nilai yang paling banyak muncul. Konsep dari modus ini berhubungan dengan kemunculan yang berulang-ulang dari suatu nilai observasi. Contoh: jika kita gunakan data produksi 11 pohon pear, maka modus produksinya adalah 326 kg. Dalam kegiatan sehari-hari, modus adalah ukuran nilai tengah yang paling jarang digunakan dibanding rata-rata atau median. Modus mungkin lebih sering digunakan pada data yang mempunyai banyak variasi dalam ukurannya, itupun untuk jumlah data yang besar. Sebagai contoh modus dari ukuran barang yang terjual sering digunakan untuk mengetahui barang yang paling disenangi konsumen. Suatu distribusi atau kelompok data mungkin tidak mempunyai modus atau mungkin mempunyai modus lebih dari satu. Distribusi yang mempunyai satu modus disebut Unimodus, yang mempunyai dua modus disebut Bimodus dan yang mempunyai modus lebih dari dua disebut Multimodus. contoh: Tentukan modus dari data dibawah ini, jika ada tentukan nilainya. a). 2, 3, 5, 7, 8. b). 2, 5, 7, 9, 9, 9, 10, 10, 11, 12. c). 2, 3, 4, 4, 4, 5, 5, 7, 7, 7, 9. solusi: Data a) tidak mempunyai modus karena semua nilai mempunyai frekuensi yang sama. Data b) mempunyai modus = 9, karena nilai observasi ini mempunyai frekuensi paling banyak. Data c) mempunyai dua modus yaitu 4 dan 7, dua nilai observasi tersebut mempunyai frekuensi palingbanyak dan sama banyak. Modus untuk data berkelompok Apabila data sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka modusnya mempunyai rumus sebagai berikut: \u200b Kelas modus adalah kelas dimana terdapat nilai modus di dalamnya. Contoh: Hitunglah nilai modus dari data kelompok pada Tabel 1. solusi: Kelas modus adalah kelompok (60 \u2013 69), karena kelompok ini mempunyai frekuensi paling banyak. \u200b Ukuran dispersi Varians Dengan ukuran nilai tengah saja kita tidak akan pernah cukup untuk memberikan ringkasan karakteristik dari sebuah set data. Bagaimana sebaran observari dari nilai rata-ratanya? Apakah observasi mempunyai dispersi atau penyimpangan yang besar dari rata-ratanya? Kita biasanya memerlukan ukuran lainnya yaitu suatu ukuran tentang dispersi atau variasi didalam data. Pada kenyataannya nilai-nilai observasi suatu populasi ada yang lebih besar dari rata-rata dan ada yang lebih kecil dari rata. Informasi ini yang biasanya merupakan keterangan tambahan mengenai karakteristik dari satu set data yaitu informasi mengenai jumlah penyimpangan dalam data. Biasanya kita tertarik dengan penyimpangan nilai-nilai observasi dalam data terhadap rata-ratanya yaitu selisihnya. Rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan yang biasa disebut dengan varians dari observasi. Simbol varians pada ukuran populasi adalah (dibaca: sigma kuadrat) dan pada ukuran sampel adalah s 2 . Simpangan baku Akar dari varians dinamakan standar deviasi atau simpangan baku. Standar deviasi merupakan ukuran simpangan yang sering digunakan dalam analisa. Nilai standar deviasi pada dasarnya menggambarkan besaran sebaran suatu kelompok data terhadap rata-ratanya atau dengan kata lain gambaran keheterogenan suatu kelompok data. Formula standar deviasi adalah sebagai berikut: \u200b Contoh: jika kita gunakan data produksi 11 pohon pear, maka varians produksinya adalah: Dari hasil perhitungan didapat varians produksi dari 11 pohon pear adalah sebesar 2.575,2 kg. sehingga standar deviasi produksinya adalah sebesar 50,75 kg. katakan kita mempunyai data produksi (dalam kg) sebanyak 10 pohon pear dengan jenis yang berbeda dengan kelompok 11 pohon pear sebelumnya, yaitu: 230 475 366 268 136 330 326 402 215 492 kelompok ini mempunyai nilai rata-rata yang sama dengan kelompok 11 pohon pear sebelumnya yaitu sebasar 324 kg. Apakah dua kelompok pohon pear tersebut mempunyai kemampuan produksi yang sama? atau dengan kata lain kelompok pohon pear mana yang lebih konsisten dalam berproduksi? Jika harus memilih jenis pohon pear mana yang lebih konsisten berproduksi, maka kita akan memilih pohon pear pada kelompok yang mempunyai nilai varians terkecil (kelompok yang lebih homogen). Varians untuk data berkelompok Formula varians untuk data berkelompok adalah sebagai berikut: \u200b contoh: kita gunakan data nilai statistik 50 mahasiswa. Koefisien variasi Standar deviasi dapat mengukur keheterogenan atau variasi suatu kelompok data. Namun jika kita ingin membandingkan dua kelompok data yang mempunyai ukuran yang berbeda, standar deviasi tidak dapat digunakan artinya standar deviasi yang lebih besar tidak selalu berarti kelompok data tersebut lebih heterogen Untuk keperluan perbandingan dua kelompok data tanpa melihat ukuran satuannya, maka dapat digunakan suatu ukuran variasi yang dinamakan koefisien variasi (CV). Rumus CV dituliskan sebagai berikut: \u200b Jika CV1 > CV2 berarti kelompok data pertama lebih bervariasi atau lebih heterogen dari pada kelompok kedua. Ukuran nilai jarak (Range) Ukuran dispersi yang paling sederhana pada suatu data numerik mungkin dengan cara menghitung selisih nilai terbesar (nilai maksimum) dengan nilai terkecil (nilai minimum). Cara ini dikenal dengan sebutan Range . Range = Nilai maksimum \u2013 Nilai minimum. Range produksi 11 pohon pear = 402 \u2013 306 = 96 Ukuran Range untuk data berkelompok Untuk data berkelompok, nilai range dihitung berdasarkan selisih antara nilai tengah kelas terakhir dengan nilai tengah pertama atau selisih batas atas kelas terakhir dengan batas bawah kelas pertama. Range = Nilai tengah kelas terakhir \u2013 Nilai tengah kelas pertama. atau Range = Bonderi atas kelas terakhir \u2013 Bonderi bawah kelas pertama. dari data nilai statistik 50 mahasiswa pada tabel 1, nilai range nya adalah: Range = 94,5 \u2013 34,5 = 60 (cara ini cenderung menghilangkan nilai ekstrim). atau Range = 99,5 \u2013 29,5 = 70. Contoh Program \u00b6 File tambahannya lihat disini import pandas as pd from scipy import stats df = pd . read_csv ( 'data.csv' , sep = \";\" ) data = { \"stats\" :[ 'min' , 'max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . mean (), stats . mode ( df [ i ]) . mode [ 0 ]] kd = pd . DataFrame ( data ) kd . style . hide_index () Pustaka \u00b6 https://digensia.wordpress.com/2012/03/15/statistik-deskriptif/","title":"statistik deskriptif"},{"location":"Tugas%201/#memahami-data","text":"","title":"Memahami Data"},{"location":"Tugas%201/#statistik-deskriptif","text":"Statistik deskriptif adalah metode dari pengorganisasian, penjumlahan, dan penyajian data dalam sebuah cara yang nyaman dan informatif, termasuk teknik grafik, dan teknik penghitungan. Pada sesi tulisan ini teknik grafik tidak akan dibahas, penyajian teknik ini akan dijelaskan pada sesi tulisan yang lain. Statistik deskriptif dapat mendeskripsikan data yang sedang dianalisis, tetapi tidak boleh menarik kesimpulan apapun dari data. Untuk pengambilan keputusan, kita perlukan cabang dari ilmu statistik lainnya yaitu statistik inferensia. Statistik inferensia akan dijelaskan pada sesi tulisan yang lain Ukuran-ukuran statistik deskriptif Ukuran statistik deskriptif dapat digolongkan menjadi dua kelompok, yaitu ukuran nilai tengah dan ukuran deviasi. Ukuran nilai tengah terdiri dari rata-rata ( mean ), median, dan modus. Sedangkan ukuran deviasi terdiri dari varians, simpangan baku, koefisien variasi, dan nilai jarak ( range ). Ukuran-ukuran statistik deskriptif tersebut akan dijelaskan penggunaannya baik untuk data tunggal maupun data berkelompok. Ukuran nilai tengah Rata-rata (Mean) Rata-rata ditulis dengan menggunakan simbol \u03bc (dibaca:\u201dmiu\u201d) untuk menyatakan rata-rata populasi, dan (dibaca: x bar) untuk menyatakan rata-rata sampel. Secara aljabar rata-rata dapat ditulis sebagai berikut: \u200b untuk rata-rata populasi \u200b \u200b dimana N adalah banyaknya populasi \u200b untuk rata-rata sampel \u200b \u200b dimana n adalah banyaknya sampel contoh: Dari 11 pohon pear menghasilkan buah dengan berat sbb (dlm Kg): 330 284 326 268 236 346 326 402 374 292 380 Hitunglah rata-rata produksi 11 pohon pear ? Jadi rata-rata produksi dari 11 pohon pear adalah 324 Kg. Rata-rata untuk data berkelompok . Apabila data sudah disajikan dalam data berkelompok seperti dalam bentuk tabel frekuensi dimana observasi-observasi dikelompokan kedalam kelas-kelas yang disebut frekuensi, maka rumus rata-ratanya adalah sebagai berikut: \u200b Contoh: Hitunglah rata-rata nilai statistik dari 50 mahasiswa pada Table 1 dibawah ini. \u200b \u200b Jadi perkiraan rata-rata nilai statistik 50 mahasiswa adalah 65,7. Median Ukuran nilai tengah lainnya yang mungkin dapat merupakan pilihan selain rata-rata adalah median. Jika data pada contoh produksi buah pear diurutkan dari nilai terkecil hingga ke nilai terbesar, maka nilai tengahnya adalah 326 kg artinya lima pohon pear mempunyai produksi dibawahnya dan lima pohon pear mempunyai produksi diatasnya. Nilai tengah inilah yang dikatakan median . Penentuan median bisa langsung didapat jika jumlah observasinya adalah ganjil, namun jika jumlah observasinya adalah genap maka akan didapat dua nilai tengah. Dalam situasi demikian, untuk mendapatkan mediannya yaitu dengan merata-ratakan dua nilai tengah yang didapat. Prosedur untuk mendapatkan median yaitu harus mengurutkan data dari yang terkecil hingga yang terbesar terlebih dahulu sebelum mengambil nilai tengahnya. Dengan kata lain median adalah data yang ke . Median untuk data berkelompok Untuk data yang sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka mediannya dapat dicari dengan rumus sebagai berikut: Kelas median adalah kelas dimana terdapat nilai median di dalamnya. Untuk menentukan kelas median bagilah seluruh observasi dengan dua artinya 50 % dari seluruh observasi terletak sebelum median dan 50 % lainnya terletak sesudahnya. Jika kita lihat tabel frekuensi (Tabel 1) maka mediannya merupakan observasi yang ke (50/2) yaitu yang ke 25. Jumlah tiga frekuensi pertama ( f1 + f2 + f3 ) yaitu 3 + 5 + 8 = 16. Untuk mencapai 25 observasi diperlukan 9 observasi lagi. 9 observasi tersebut dapat dipenuhi dari frekuensi keempat ( f4 ) karena jumlah observasi f4 ada sebanyak 14 observasi. Jadi median terletak pada kelas keempat atau kelas (60 \u2013 69) dengan kata lain kelas keempat adalah kelas median. contoh: Hitunglah nilai median dari data kelompok pada Tabel 1. solusi: \u200b Jadi mediannya, \u200b Pertanyaan yang mungkin timbul adalah jika kita punya data aslinya, apakah nilai median yang sebenarnya adalah 66,33? jawabannya belum tentu, karena cara ini adalah cara interpolasi dimana data aslinya memang tidak diketahui, yang ada adalah data sudah dalam bentuk tabel frekuensi atau sudah dikelompokkan. Walaupun hasil interpolasi ini mungkin tidak tepat, namun cara ini memberikan hasil yang mendekati nilai median yang sebenarnya. Modus Modus dari suatu kelompok observasi adalah nilai observasi yang mempunyai frekuensi pemunculan paling banyak atau dengan kata lain yaitu nilai yang paling banyak muncul. Konsep dari modus ini berhubungan dengan kemunculan yang berulang-ulang dari suatu nilai observasi. Contoh: jika kita gunakan data produksi 11 pohon pear, maka modus produksinya adalah 326 kg. Dalam kegiatan sehari-hari, modus adalah ukuran nilai tengah yang paling jarang digunakan dibanding rata-rata atau median. Modus mungkin lebih sering digunakan pada data yang mempunyai banyak variasi dalam ukurannya, itupun untuk jumlah data yang besar. Sebagai contoh modus dari ukuran barang yang terjual sering digunakan untuk mengetahui barang yang paling disenangi konsumen. Suatu distribusi atau kelompok data mungkin tidak mempunyai modus atau mungkin mempunyai modus lebih dari satu. Distribusi yang mempunyai satu modus disebut Unimodus, yang mempunyai dua modus disebut Bimodus dan yang mempunyai modus lebih dari dua disebut Multimodus. contoh: Tentukan modus dari data dibawah ini, jika ada tentukan nilainya. a). 2, 3, 5, 7, 8. b). 2, 5, 7, 9, 9, 9, 10, 10, 11, 12. c). 2, 3, 4, 4, 4, 5, 5, 7, 7, 7, 9. solusi: Data a) tidak mempunyai modus karena semua nilai mempunyai frekuensi yang sama. Data b) mempunyai modus = 9, karena nilai observasi ini mempunyai frekuensi paling banyak. Data c) mempunyai dua modus yaitu 4 dan 7, dua nilai observasi tersebut mempunyai frekuensi palingbanyak dan sama banyak. Modus untuk data berkelompok Apabila data sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka modusnya mempunyai rumus sebagai berikut: \u200b Kelas modus adalah kelas dimana terdapat nilai modus di dalamnya. Contoh: Hitunglah nilai modus dari data kelompok pada Tabel 1. solusi: Kelas modus adalah kelompok (60 \u2013 69), karena kelompok ini mempunyai frekuensi paling banyak. \u200b Ukuran dispersi Varians Dengan ukuran nilai tengah saja kita tidak akan pernah cukup untuk memberikan ringkasan karakteristik dari sebuah set data. Bagaimana sebaran observari dari nilai rata-ratanya? Apakah observasi mempunyai dispersi atau penyimpangan yang besar dari rata-ratanya? Kita biasanya memerlukan ukuran lainnya yaitu suatu ukuran tentang dispersi atau variasi didalam data. Pada kenyataannya nilai-nilai observasi suatu populasi ada yang lebih besar dari rata-rata dan ada yang lebih kecil dari rata. Informasi ini yang biasanya merupakan keterangan tambahan mengenai karakteristik dari satu set data yaitu informasi mengenai jumlah penyimpangan dalam data. Biasanya kita tertarik dengan penyimpangan nilai-nilai observasi dalam data terhadap rata-ratanya yaitu selisihnya. Rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan yang biasa disebut dengan varians dari observasi. Simbol varians pada ukuran populasi adalah (dibaca: sigma kuadrat) dan pada ukuran sampel adalah s 2 . Simpangan baku Akar dari varians dinamakan standar deviasi atau simpangan baku. Standar deviasi merupakan ukuran simpangan yang sering digunakan dalam analisa. Nilai standar deviasi pada dasarnya menggambarkan besaran sebaran suatu kelompok data terhadap rata-ratanya atau dengan kata lain gambaran keheterogenan suatu kelompok data. Formula standar deviasi adalah sebagai berikut: \u200b Contoh: jika kita gunakan data produksi 11 pohon pear, maka varians produksinya adalah: Dari hasil perhitungan didapat varians produksi dari 11 pohon pear adalah sebesar 2.575,2 kg. sehingga standar deviasi produksinya adalah sebesar 50,75 kg. katakan kita mempunyai data produksi (dalam kg) sebanyak 10 pohon pear dengan jenis yang berbeda dengan kelompok 11 pohon pear sebelumnya, yaitu: 230 475 366 268 136 330 326 402 215 492 kelompok ini mempunyai nilai rata-rata yang sama dengan kelompok 11 pohon pear sebelumnya yaitu sebasar 324 kg. Apakah dua kelompok pohon pear tersebut mempunyai kemampuan produksi yang sama? atau dengan kata lain kelompok pohon pear mana yang lebih konsisten dalam berproduksi? Jika harus memilih jenis pohon pear mana yang lebih konsisten berproduksi, maka kita akan memilih pohon pear pada kelompok yang mempunyai nilai varians terkecil (kelompok yang lebih homogen). Varians untuk data berkelompok Formula varians untuk data berkelompok adalah sebagai berikut: \u200b contoh: kita gunakan data nilai statistik 50 mahasiswa. Koefisien variasi Standar deviasi dapat mengukur keheterogenan atau variasi suatu kelompok data. Namun jika kita ingin membandingkan dua kelompok data yang mempunyai ukuran yang berbeda, standar deviasi tidak dapat digunakan artinya standar deviasi yang lebih besar tidak selalu berarti kelompok data tersebut lebih heterogen Untuk keperluan perbandingan dua kelompok data tanpa melihat ukuran satuannya, maka dapat digunakan suatu ukuran variasi yang dinamakan koefisien variasi (CV). Rumus CV dituliskan sebagai berikut: \u200b Jika CV1 > CV2 berarti kelompok data pertama lebih bervariasi atau lebih heterogen dari pada kelompok kedua. Ukuran nilai jarak (Range) Ukuran dispersi yang paling sederhana pada suatu data numerik mungkin dengan cara menghitung selisih nilai terbesar (nilai maksimum) dengan nilai terkecil (nilai minimum). Cara ini dikenal dengan sebutan Range . Range = Nilai maksimum \u2013 Nilai minimum. Range produksi 11 pohon pear = 402 \u2013 306 = 96 Ukuran Range untuk data berkelompok Untuk data berkelompok, nilai range dihitung berdasarkan selisih antara nilai tengah kelas terakhir dengan nilai tengah pertama atau selisih batas atas kelas terakhir dengan batas bawah kelas pertama. Range = Nilai tengah kelas terakhir \u2013 Nilai tengah kelas pertama. atau Range = Bonderi atas kelas terakhir \u2013 Bonderi bawah kelas pertama. dari data nilai statistik 50 mahasiswa pada tabel 1, nilai range nya adalah: Range = 94,5 \u2013 34,5 = 60 (cara ini cenderung menghilangkan nilai ekstrim). atau Range = 99,5 \u2013 29,5 = 70.","title":"Statistik Deskriptif"},{"location":"Tugas%201/#contoh-program","text":"File tambahannya lihat disini import pandas as pd from scipy import stats df = pd . read_csv ( 'data.csv' , sep = \";\" ) data = { \"stats\" :[ 'min' , 'max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . mean (), stats . mode ( df [ i ]) . mode [ 0 ]] kd = pd . DataFrame ( data ) kd . style . hide_index ()","title":"Contoh Program"},{"location":"Tugas%201/#pustaka","text":"https://digensia.wordpress.com/2012/03/15/statistik-deskriptif/","title":"Pustaka"},{"location":"Tugas%202/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm . Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan , where \u03bc x The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik.","title":"Mengukur Jarak"},{"location":"Tugas%202/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Tugas%202/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak Tipe Numerik"},{"location":"Tugas%202/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"Tugas%202/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan","title":"Manhattan distance"},{"location":"Tugas%202/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"Tugas%202/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan","title":"Average Distance"},{"location":"Tugas%202/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"Tugas%202/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm .","title":"Chord distance"},{"location":"Tugas%202/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"Tugas%202/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan","title":"Cosine measure"},{"location":"Tugas%202/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan , where \u03bc x The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation"},{"location":"Tugas%202/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"Tugas%202/#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical"},{"location":"Tugas%202/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"Tugas%202/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"Tugas%202/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"Tugas%202/#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"Tugas%202/#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"Tugas%203/","text":"Missing Values using KNN \u00b6 KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan. Kalibrasi Parameter KNN \u00b6 Jumlah tetangga yang harus dicari \u00b6 Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan. Metode agregasi untuk digunakan \u00b6 Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal Normalisasi data \u00b6 Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan. Atribut numerik jarak \u00b6 Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...). Atribut kategorikal jarak \u00b6 tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from list df = pd . DataFrame ( dict ) # using isnull() function df . isnull () First Score Second Score Third Score 0 False False True 1 False False False 2 True False False 3 False True False","title":"Missing Value"},{"location":"Tugas%203/#missing-values-using-knn","text":"KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan.","title":"Missing Values using KNN"},{"location":"Tugas%203/#kalibrasi-parameter-knn","text":"","title":"Kalibrasi Parameter KNN"},{"location":"Tugas%203/#jumlah-tetangga-yang-harus-dicari","text":"Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan.","title":"Jumlah tetangga yang harus dicari"},{"location":"Tugas%203/#metode-agregasi-untuk-digunakan","text":"Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal","title":"Metode agregasi untuk digunakan"},{"location":"Tugas%203/#normalisasi-data","text":"Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan.","title":"Normalisasi data"},{"location":"Tugas%203/#atribut-numerik-jarak","text":"Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...).","title":"Atribut numerik jarak"},{"location":"Tugas%203/#atribut-kategorikal-jarak","text":"tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from list df = pd . DataFrame ( dict ) # using isnull() function df . isnull () First Score Second Score Third Score 0 False False True 1 False False False 2 True False False 3 False True False","title":"Atribut kategorikal jarak"},{"location":"Tugas%205%20-%20Clustering/","text":"CLUSTERING \u00b6 Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM). Clustering dengan pendekatan partisi \u00b6 1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 \u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means \u00b6 Metode K-Modes \u00b6 \u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Metode K-Prototype \u00b6 \u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Algoritma K-Prototype \u00b6 \u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. Rumus K-Prototype \u00b6 2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Clustering dengan Pendekatan Hirarki \u00b6 \u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa. Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM) \u00b6 \u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"Tugas%205%20-%20Clustering/#clustering","text":"Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM).","title":"CLUSTERING"},{"location":"Tugas%205%20-%20Clustering/#clustering-dengan-pendekatan-partisi","text":"1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Clustering dengan pendekatan partisi"},{"location":"Tugas%205%20-%20Clustering/#algoritma-k-means","text":"\u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Algoritma K-Means"},{"location":"Tugas%205%20-%20Clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"Tugas%205%20-%20Clustering/#metode-k-modes","text":"\u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"Tugas%205%20-%20Clustering/#metode-k-prototype","text":"\u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"Tugas%205%20-%20Clustering/#algoritma-k-prototype","text":"\u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek.","title":"Algoritma K-Prototype"},{"location":"Tugas%205%20-%20Clustering/#rumus-k-prototype","text":"2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya.","title":"Rumus K-Prototype"},{"location":"Tugas%205%20-%20Clustering/#clustering-dengan-pendekatan-hirarki","text":"\u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa.","title":"Clustering dengan Pendekatan Hirarki"},{"location":"Tugas%205%20-%20Clustering/#clustering-dengan-pendekatan-automatic-mapping-self-organising-mapsom","text":"\u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM)"},{"location":"Tugas%206%20fuzzy%20clustering/","text":"fuzzy clustering \u00b6 pengertian fuzzy clustering: \u00b6 Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0} ; FPC = {1:.2f} ' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"Fuzzy Clustering"},{"location":"Tugas%206%20fuzzy%20clustering/#fuzzy-clustering","text":"","title":"fuzzy clustering"},{"location":"Tugas%206%20fuzzy%20clustering/#pengertian-fuzzy-clustering","text":"Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0} ; FPC = {1:.2f} ' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"pengertian fuzzy clustering:"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/","text":"Regresi Linier Sederhana dan Berganda \u00b6 Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y). Regresi Linier Sederhana \u00b6 Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut : Regresi Linier Berganda \u00b6 Regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan) Contoh kasus dengan penghitungan manual \u00b6 Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi. Langkah 1 : Penentuan Tujuan \u00b6 Tujuan : Memprediksi Jumlah cacat produksi jika suhu ruangan tidak terkendali Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat \u00b6 Varibel Faktor Penyebab (X) : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi Langkah 3 : Pengumpulan Data \u00b6 Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14 Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya \u00b6 Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya : Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana \u00b6 Langkah 6 : Buat Model Persamaan Regresi \u00b6 Y = a + bX Y = -24,38 + 1,45X Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat \u00b6 I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C Contoh kasus dengan penghitungan sklearn \u00b6 Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Regresi Linier Sederhana dan Berganda"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#regresi-linier-sederhana-dan-berganda","text":"Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y).","title":"Regresi Linier Sederhana dan Berganda"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#regresi-linier-sederhana","text":"Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut :","title":"Regresi Linier Sederhana"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#regresi-linier-berganda","text":"Regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan)","title":"Regresi Linier Berganda"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#contoh-kasus-dengan-penghitungan-manual","text":"Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi.","title":"Contoh kasus dengan penghitungan manual"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#langkah-1-penentuan-tujuan","text":"Tujuan : Memprediksi Jumlah cacat produksi jika suhu ruangan tidak terkendali","title":"Langkah 1 : Penentuan Tujuan"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#langkah-2-identifikasikan-variabel-penyebab-dan-akibat","text":"Varibel Faktor Penyebab (X) : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi","title":"Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#langkah-3-pengumpulan-data","text":"Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14","title":"Langkah 3 : Pengumpulan Data"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#langkah-4-hitung-x2-y2-xy-dan-total-dari-masing-masingnya","text":"Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya :","title":"Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#langkah-5-hitung-a-dan-b-berdasarkan-rumus-regresi-linear-sederhana","text":"","title":"Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#langkah-6-buat-model-persamaan-regresi","text":"Y = a + bX Y = -24,38 + 1,45X","title":"Langkah 6 : Buat Model Persamaan Regresi"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#langkah-7-lakukan-prediksi-atau-peramalan-terhadap-variabel-faktor-penyebab-atau-variabel-akibat","text":"I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C","title":"Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat"},{"location":"Tugas%207%20-%20Regresi%20Linier%20Sederhana%20dan%20Berganda/#contoh-kasus-dengan-penghitungan-sklearn","text":"Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Contoh kasus dengan penghitungan sklearn"},{"location":"tugas1/","text":"Error in numerical computation \u00b6 \u00b6 ERROR \u00b6 \u00b6 Error merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah penting, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi. NILAI ERROR \u00b6 \u00b6 Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya suatu kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya suatukesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif . Absolut error \u00b6 \u00b6 Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan Ea=|X\u2212x|Ea=|X\u2212x| kesalahan Relatif \u00b6 \u00b6 Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. Er=|Xt\u2212Xa/Xt|Er=|Xt\u2212Xa/Xt| PENYEBAB TERJADINYA ERROR \u00b6 \u00b6 Dibedakan dalam beberapa macam : 1.Round-off-errors \u00b6 \u00b6 Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja. 2.Truncation errors \u00b6 \u00b6 Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi. 3.Inherent error \u00b6 \u00b6 Terjadi akibat kekeliruan dalam menyalin data, salah membaca skala atau kesalahan karena kurangnya pengertian mengenai hukum-hukum fisik dari data yang diukur. Kesalahan ini sering terjadi karena faktor human errorDEFINISI MACLAURIN Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi. Berikut algoritma dari maclaurin \u00b6 Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi Listing Program \u00b6 membuat program supaya dapaat mengekspansi bilangan e^2x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math x = 4 coba = 1 a = 0 b = 1 while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) coba = f_y - f_x a += 1 b += 1 print ( coba ) output: 1.0 8.0 32.0 85.33333333333333 170.66666666666669 273.0666666666666 364.08888888888896 416.1015873015872 416.1015873015872 369.8680776014112 295.89446208112895 215.195972422639 143.46398161509296 88.28552714774924 50.448872655856576 26.90606541645684 13.45303270822842 6.330838921519444 2.8137061873417224 1.184718394670199 0.47388735786807956 0.18052851728316455 0.06564673355751438 0.022833646454728296 0.0076112154847578495 0.0024355889549951826 0.0007494119863622473 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Error in numerical computation[\u00b6](https://maretakurniasari.github.io/mareta.kurnia.github.io/tugas1komnum/#error-in-numerical-computation)"},{"location":"tugas1/#error-in-numerical-computation","text":"","title":"Error in numerical computation\u00b6"},{"location":"tugas1/#error","text":"Error merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah penting, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi.","title":"ERROR\u00b6"},{"location":"tugas1/#nilai-error","text":"Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya suatu kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya suatukesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif .","title":"NILAI ERROR\u00b6"},{"location":"tugas1/#absolut-error","text":"Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan Ea=|X\u2212x|Ea=|X\u2212x|","title":"Absolut error\u00b6"},{"location":"tugas1/#kesalahan-relatif","text":"Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. Er=|Xt\u2212Xa/Xt|Er=|Xt\u2212Xa/Xt|","title":"kesalahan Relatif\u00b6"},{"location":"tugas1/#penyebab-terjadinya-error","text":"Dibedakan dalam beberapa macam :","title":"PENYEBAB TERJADINYA ERROR\u00b6"},{"location":"tugas1/#1round-off-errors","text":"Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja.","title":"1.Round-off-errors\u00b6"},{"location":"tugas1/#2truncation-errors","text":"Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi.","title":"2.Truncation errors\u00b6"},{"location":"tugas1/#3inherent-error","text":"Terjadi akibat kekeliruan dalam menyalin data, salah membaca skala atau kesalahan karena kurangnya pengertian mengenai hukum-hukum fisik dari data yang diukur. Kesalahan ini sering terjadi karena faktor human errorDEFINISI MACLAURIN Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi.","title":"3.Inherent error\u00b6"},{"location":"tugas1/#berikut-algoritma-dari-maclaurin","text":"Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi","title":"Berikut algoritma dari maclaurin"},{"location":"tugas1/#listing-program","text":"membuat program supaya dapaat mengekspansi bilangan e^2x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math x = 4 coba = 1 a = 0 b = 1 while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) coba = f_y - f_x a += 1 b += 1 print ( coba ) output: 1.0 8.0 32.0 85.33333333333333 170.66666666666669 273.0666666666666 364.08888888888896 416.1015873015872 416.1015873015872 369.8680776014112 295.89446208112895 215.195972422639 143.46398161509296 88.28552714774924 50.448872655856576 26.90606541645684 13.45303270822842 6.330838921519444 2.8137061873417224 1.184718394670199 0.47388735786807956 0.18052851728316455 0.06564673355751438 0.022833646454728296 0.0076112154847578495 0.0024355889549951826 0.0007494119863622473 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Listing Program"}]}